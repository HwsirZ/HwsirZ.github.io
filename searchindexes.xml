<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>机器人自主导航（一）</title><url>/embodied-intelligence/%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%87%AA%E4%B8%BB%E5%AF%BC%E8%88%AA%E4%B8%80/</url><categories/><tags/><content type="html"><![CDATA[  综述《A Comprehensive Review on Autonomous Navigation》
Link: https://doi.org/10.1145/3727642本文是对上述综述内容的部分翻译与总结
机器人技术的应用领域医疗
军事
工业
太空
农业
&hellip;&hellip;
一：第一台自主移动机器人ShakeyShakey 机器人是世界上第一台移动机器人，由查理·罗森（Charlie Rosen）领导的美国斯坦福研究所于1956年至1972年研制而成。Shakey首次全面应用了人工智能技术，能够自主进行感知、环境建模、行为规划并执行任务，如寻找木箱并将其推到指定位置。它装备了电子摄像机、三角测距仪、碰撞传感器以及驱动电机，并通过无线通信系统由两台计算机控制，如下图所示。
二：第一辆自动驾驶汽车Stanford Cart据了解，Stanford Cart使用一台帧率为1Hz的黑白相机，可以在道路上沿着一条不间断的白线行驶15m，当然移动速度很慢，以下是搜到的实物图
三：综述描述的主要内容传统避障方法和现代避障方法（基于强化学习与深度学习）
著名的SLAM方法
流行的机器人模拟器以及与机器人操作系统（ROS）的适配性
基于操作环境的移动机器人平台类型与特性
著名的传感器融合方法（卡尔曼滤波及其扩展以及粒子滤波器）
开源SLAM数据集
3.1 路径规划方法(一)环境信息的获取绝对定位传感器GPS
障碍物检测传感器
视觉传感器：RGBD摄像机
范围传感器：雷达、脉冲激光、超声波
相对定位传感器
惯性测量单元（inertial measurement unit IMU）
旋转编码器（rotary encoder）
路径规划方法分类：
传统方法&mdash; $ A^* $ 算法、Dijkstra算法
基于AI&mdash; 强化学习(RL)、遗传算法
混合方法&mdash; 传统方法与AI方法的结合
另一种分类：
几何模型搜索方法 （Geometric Model Search Methods）
随机抽样方法 （Random Sampling Methods）
生物启发方法 （Bio-inspired Methods）
人工智能方法 （Artificial Potential Filed Method）
（二）路径规划方法介绍1.Geometric Model Search Methods（几何搜索方法）  ]]></content></entry><entry><title>人机均衡——“人类与机器的边界”</title><url>/post/%E4%BA%BA%E6%9C%BA%E5%9D%87%E8%A1%A1%E4%BA%BA%E7%B1%BB%E4%B8%8E%E6%9C%BA%E5%99%A8%E7%9A%84%E8%BE%B9%E7%95%8C/</url><categories/><tags/><content type="html">  人机均衡 关于AI游戏智能体引发的思考，人类与计算机应该达到怎样的平衡？最初是要做一个机器学习的课程设计，选择了强化学习玩斗地主，从对DouZero模型的学习中，有了一些相关的思考。如今，智能与人类生活紧密融合，但是人类与计算机或者说智能设备的边界又在哪里呢，计算机所控制的智能体能否真正完全替代人类呢？
先从游戏说起，对于游戏，在计算机领域，不可避免的想到Alpha Go，这个强大的围棋AI，围棋游戏是一个完全可观测环境，对于围棋，所谓完全可观测环境就是所有信息已经在棋盘上显示出来了，而斗地主这个游戏属于部分可观测环境，也就是你无法知道其他玩家的手牌。试想，如果围棋也是一个多人游戏，假定为四人吧，2v2，那么在完全可观测环境下，队友的完美合作完全是有可能的。但是对于斗地主这样的部分可观测环境呢，两个农民也只知道自己的手牌。对于AI程序，当然可以从训练中计算出某一个出牌决策的优劣度，但是面对随机发放的手牌，即使再好的出牌策略可能也无能为力。可以想象到，由于洗牌是随机的，每个人拿到的手牌的好坏程度概率是一样的，因此很难说训练的地主模型一定优于农民模型。或者农民模型一定优于地主模型。对于这种部分可观测娱乐游戏，尤其是带有随机性的，机器对人的替代似乎必要性很少。斗地主这个游戏还不是很明显，接下来将从另一个游戏说起。
从一个山东朋友那里了解到保皇这个游戏，四副扑克牌，五人游戏，一个皇帝，一个侍卫，三个平民。这比斗地主更具有游戏性与合作性。玩过三国杀的玩家应该很熟悉，主公、反贼、忠臣。由于目前斗地主的AI已经训练的很好的，不管是？DouZero还是使用ResNet训练的DouZero，还是PredictDou等模型。所以我在想能不能仿照着做一个保皇游戏的AI，首先让我望而却步的是保皇游戏中的诸多规则，进贡、明保、暗保等。上述已经是一个月之前的事情了。而今天突然想到，先假设已经训练了这样一个玩保皇游戏的模型，那么，想想一下，这个游戏里所体现的合作、欺骗、竞争又该怎么体现呢，人们玩这类游戏的初衷便是体验游戏的过程，保子(侍卫)可能享受冒充平民的乐趣，平民也在提放着保子，我认为这正是人类玩这个游戏的初衷。就好像三国杀，忠臣、反贼的扮演本身带给了玩家乐趣。而机器是没有这种感情的，人类和机器玩这种游戏，当然也无法从机器身上感受到任何感情，也许对于如今快速发展的大语言模型，是可以模拟这种感 …  </content></entry><entry><title>Linux常用命令汇总</title><url>/linux/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB/</url><categories/><tags/><content type="html">  一：远程Linux文件传输SCP 传输SCP（Secure Copy）是一种安全的文件传输方法，可以在不同主机之间传输文件。SCP命令通过SSH协议加密传输文件，提高了安全性，但相对速度会慢一些。
1 2 3 4 # 从本地复制文件到远程主机 scp [local directory] [username]@[ip adress]:[remote directory] # 从远程主机复制文件到本地 scp -rp [ip adress]:[directory] [local adress]   </content></entry><entry><title>Windows下载wsl_linux子系统并配置可视化界面</title><url>/linux/windows%E4%B8%8B%E8%BD%BDwsl_linux%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%B9%B6%E9%85%8D%E7%BD%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2/</url><categories/><tags/><content type="html">  由于很多项目都是在linux系统下做开发，而目前很多人使用的还都是windows系统的笔记本电脑或者主机电脑，因此windows下的linux子系统是一个不错的选择，但是配置方面仍然会遇到一些问题，本文章主要记录linux子系统的安装以及可视化GUI的配置以及使用
一、安装前的准备确认 Windows 版本：WSL 2（推荐版本）需要：
Windows 10 2004 及以上版本（内部版本 19041+）
Windows 11 任意版本
检查方法：按下 Win + R，输入 winver 回车，查看 “内部版本号”。
开启硬件虚拟化：WSL 依赖硬件虚拟化技术（如 Intel VT-x/AMD-V），需在 BIOS 中开启（多数电脑默认开启，若后续安装报错，再按此步骤排查）。
二、安装 WSL（推荐自动安装，10 分钟搞定）步骤 1：用命令自动安装（最简单）以 管理员身份 打开 PowerShell 或 命令提示符（CMD）：
按下 Win 键，搜索 “PowerShell”，右键选择 “以管理员身份运行”。 输入以下命令，自动启用 WSL 功能并安装默认 Linux 发行版（通常是 Ubuntu）：
powershell
1 wsl --install 这个命令会自动完成：
启用 “适用于 Linux 的 Windows 子系统” 和 “虚拟机平台” 两个核心功能； 下载并安装 WSL 2 内核； 安装默认的 Linux 发行版（Ubuntu，社区最活跃，适合新手）。 等待安装完成后，重启电脑（必须重启，否则功能不生效）。
步骤 2：手动选择发行版（可选，若需其他 Linux 系统）如果想安装 Ubuntu 以外的发行版（如 Debian、Fedora）：
先执行命令查看可用发行版：
powershell
1 wsl --list --online 会显示类似列表：
plaintext
1 2 3 4 5 以下是可安装的有效分发版: NAME FRIENDLY NAME Ubuntu Ubuntu Debian Debian GNU/Linux kali-linux Kali Linux Rolling 安装指定发行版（例如 Debian）：
powershell
1 wsl --install -d Debian 三、初始化 Linux 系统（首次启 …  </content></entry><entry><title>具身智能Habitat库安装使用(一)</title><url>/embodied-intelligence/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BDhabitat%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%E4%B8%80/</url><categories/><tags/><content type="html">  一：Habitat-sim 、 Habitat-lab and Habitat-MARLHabitat是一个具身智能仿真平台，核心组件包括
Habitat-Sim：高性能 3D 模拟器，支持物理引擎（如 Bullet）、多模态传感器（RGB、深度图、语义分割）和真实场景数据集（如 MatterPort3D）。 Habitat-Lab：模块化库，用于定义任务（如导航、指令遵循）、配置代理（机器人或人形）和训练算法（强化学习或模仿学习）。 Habitat-MARL：扩展模块，支持多智能体协作训练，适用于多机器人导航、资源分配等场景。 二：Habitat-Sim的安装目前Habitat-Sim应该只有Linux版本与Mac版本，而且Mac版本也只有基本功能，在Windows上确实可以通过cmake重新编译来使用，但是难免会有兼容性问题，因此我尝试在Windows的linux子系统中使用。这个库毕竟是一个3D场景的模拟库，需要可视化界面，因此尝试配置linux的Gui后使用.
1. 安装方式编译安装
直接下载
这里直接下载使用
habitat库的地址，这个地址中描述了通过Conda Packages安装habitat-sim，下面是对这个过程的复述，但是按照这个步骤，最后运行测试用例似乎有些问题，在Issues中有人提出了这个问题，就是python运行后会报错：from magnum import text 错误，也就是无法找到text子模块
1.1 准备conda env1 2 3 4 5 6 7 conda create -n habitat python=3.9 cmake=3.14.0 conda activate habitat # 克隆仓库 git clone https://github.com/facebookresearch/habitat-sim.git cd habitat # 安装所需要的库 pip install -r requirements.txt 1.2 通过 conda 安装 habitat-smi这一步如果按照git仓库的readme，最后运行可能失败，所以选择以下方式
理论上来说提供了四种安装方式，其中有针对命令行也就是无可视化界面的headless版本，我这里选择有可视化界面的withbullet版本
1 conda install habitat-sim withbullet -c conda-forge -c aihabitat-nightly 这里我下载有些缓慢，另外habitat-sim本身的库下载，不知道什么原因，进度条不显示进度，但是确实在下载，耐心等待即可
1.3 下载 (testing) 3D scenes and example objects1 2 3 4 # 这里把后面的下载路径改为自己的就可以，下载到habitat文件夹的data下就行 python -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path /path/to/data/ python -m habitat_sim.utils.datasets_download --uids habitat_example_objects --data-path /path/to/data/ 1.4 交互测试有两种交互测试方式，c++与pyhon，既然选择了python开发，肯定用python
1 python examples/viewer.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb 运行效果
1.5 Physical interactions下载数据
1 2 3 4 5 6 #NOTE: by default, data will be downloaded into habitat-sim/data/. Optionally modify the data path by adding: `--data-path /path/to/data/` # with conda install python -m habitat_sim.utils.datasets_download --uids replica_cad_dataset # with source (from inside habitat_sim/) python src_python/habitat_sim/utils/datasets_download.py --uids replica_cad_dataset 运行命令
1 python examples/viewer.py --dataset data/replica_cad/replicaCAD.scene_dataset_config.json --scene apt_1 运行效果
  </content></entry><entry><title>蒙特卡洛方法</title><url>/reinforcement-learning/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</url><categories/><tags/><content type="html"><![CDATA[  蒙特卡洛方法基于对样本回报求平均值的办法来解决问题，与环境进行真实或者模拟的交互得到状态、动作、奖励的样本序列，不需要对环境有完全的了解，在概率上，通过与环境交互，得到样本，用足够多的样本去估计总体，得到一个近似。
一：蒙特卡洛的预测在给定策略的情况下，用蒙特卡洛方法学习状态-价值函数。
一个状态的价值等于从这个状态开始的期望回报——期望的累计未来折扣奖励。
我们用$V_\pi$表示在策略$\pi$下状态s的价值，通过不断模拟采样计算平均值，这个值会收敛于状态的价值的期望。
两种蒙特卡洛方法遵循策略$\pi$我们得到了一些回合，每个回合种都有状态S，对于第一次的S，我们称为首次访问，S可能被访问多次，我们只记录第一次，根据后续再次遇到S是否记录，可以将蒙特卡洛方法分为两种
1.首次访问型蒙特卡洛方法1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 &#34;&#34;&#34; 伪代码 输入: 给定的策略π 初始化: 对所有 s∈S，任意 V(s)∈R Returns(s) ← 一个空的列表，对所有 s∈S 一直循环（对每一个回合）: 使用 π 生成一个回合：S0,A0,R1,S1,A1,R2,…,ST−1,AT−1,RT G←0 对于回合中的每一步循环，t=T−1,T−2,…,0: G←γG+R(t+1) 除非 St出现在 S0,S1,…,St−1中: 将 G添加到列表 Returns(s)中 V(s)←average(Returns(s)) &#34;&#34;&#34; policy = give_pi gamma = 0.01 # 超参数，是对近期序列的一个增益计算 def init(states): returns =[s:[] for s in states] v = {s: 0.0 for s in states} return returns, v def generate(policy): &#34;&#34;&#34;根据policy生成回合序列&#34;&#34;&#34; states = [s0, s1, s2, ......, sn] # 假设的状态序列（S0, S1, S2） actions = [a0, a1, a2, ......, an] # 对应的动作 rewards = [r1, r2, r3, ......, rn] # 对应的奖励（S0→R1，S1→R2，S2→R3） return [states, actions, rewards] all_states = [s0, s1, s2, ...] returns, v = init(all_states) # 初始化 while True: episode = generate(policy) states, actions, rewards = episode T = len(states) # 回合长度 gain = 0 # 增益 mark = [] # 记录当前回合中已出现过的状态（用于判断首次访问） for t in range(T-1,-1,-1): St = states[t] # 当前状态 Rt = rewards[t] gain = gamma*gain + Rt if St not in mark: # 将G添加到St对应的回报列表 returns[St].append(G) # 更新St的价值为其所有回报的平均值 v[St] = sum(returns[St]) / len(returns[St]) # 将当前状态St加入mark（供后续更早的步骤判断是否重复） mark.append(St) 2.每次访问蒙特卡洛方法  ]]></content></entry><entry><title>YoloV10入门</title><url>/visual-detection-model/yolov10%E5%85%A5%E9%97%A8/</url><categories/><tags/><content type="html">  一：YOLO的版本演进您观察得非常仔细！这里涉及到 YOLO 发展史上一个重要的演变过程。您的问题引出了一个关键点：YOLO 的“发明者”和后来的“主要发展者”发生了变化。
让我为您清晰地梳理一下：
YOLO 的原始发明者核心人物：Joseph Redmon 和 Ali Farhadi。 所在机构：当时他们在华盛顿大学。 贡献：他们在 2015 年的 CVPR 会议上发表了第一篇 YOLO 论文，创造了 YOLO 算法本身。从 YOLOv1 到 YOLOv3，主要是由 Joseph Redmon 领导开发的。 后续：由于 YOLO 技术可能被应用于军事、隐私侵犯等伦理问题，Joseph Redmon 在 2020 年宣布停止对 YOLO 的研究。这之后，YOLO 的发展进入了“百家争鸣”的时代。 版本 主要开发团队/机构 特点与备注 YOLOv1-v3 Joseph Redmon（华盛顿大学） YOLO 的创始版本 YOLOv4, v7 Alexey Bochkovskiy 等 侧重于算法创新，性能强劲 YOLOv5, v8 Ultralytics 侧重工程应用，极度易用，最流行 YOLOv9 Alexey Bochkovskiy 等 &amp;amp; Ultralytics 两大团队合作产物 YOLOv10 清华大学 率先发布，后被 Ultralytics 集成 二：  </content></entry><entry><title>K臂赌博机的增量实现以及追踪非平衡问题</title><url>/reinforcement-learning/k%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA%E7%9A%84%E5%A2%9E%E9%87%8F%E5%AE%9E%E7%8E%B0%E4%BB%A5%E5%8F%8A%E8%BF%BD%E8%B8%AA%E9%9D%9E%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/</url><categories/><tags/><content type="html"><![CDATA[  一：增量实现以及非平稳问题所谓增量实现，就是对$Q_n$的计算方法，之前的计算是对过去所有奖励的一个平均值来估计奖励，而增量更新中，对于过去的奖励，有不同的权重，此时，$Q_n$的计算是过去所有奖励的权重和。
首先是数学公式的推到，先推到对于之前求平均值的公式
$$ \begin{align} Q_{n+1} &amp;amp; = \frac{R_1+R2+\dots +R_{n}}{n} \\ &amp;amp; = \frac{1}{n} \sum_{i=1}^{n} R_i \\ &amp;amp; = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_{i}) \\ &amp;amp; = \frac{1}{n} (R_n + (n-1)\frac{1}{n-1}Q_n\sum_{i=1}^{n-1} R_{i}) \\ &amp;amp; = \frac{1}{n} (R_n + (n-1)Q_n) \\ &amp;amp; = Q_n + \frac{1}{n}(R_n - Q_n) \end{align} $$
将$\frac{1}{n}$替换为$\alpha, \quad \alpha \in (0,1] \quad$即表示增量更新的公式，用于非平稳问题的求解，此时的公式可以表示为：
$$ \begin{align} Q_{n+1} &amp;amp; = Q_n + \alpha(R_n - Q_n) \\ &amp;amp; = \alpha R_n + (1-\alpha)Q_n \\ &amp;amp; = (1-\alpha)^{n}Q_1 + \sum_{i=1}^{n}\alpha (1 - \alpha )^{n-i}R_i \end{align} $$
权重之和$\quad (1-\alpha )^n + \sum_{i=1}^{n}\alpha(1-\alpha)^{n-i} = 1 \quad$ (使用等比数列的求和公式进行计算推导)，有时也称为指数新近加权平均值。
这里还有一个收敛性的推导：
$\sum_{n=1}^{\infty}\alpha_n(a) = \infty \text{ 以及 } \sum_{n=1}^{\infty}\alpha_n^{2}(a) &amp;lt; \infty$
这个的计算也是对上述公式进行展开计算，看最后的Q值能否收敛到期望值，用到了一个 …  ]]></content></entry><entry><title>K臂平稳赌博机</title><url>/reinforcement-learning/k%E8%87%82%E5%B9%B3%E7%A8%B3%E8%B5%8C%E5%8D%9A%E6%9C%BA/</url><categories/><tags/><content type="html">   💡 无法显示？点击下载  </content></entry><entry><title>马尔可夫决策过程</title><url>/reinforcement-learning/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url><categories/><tags/><content type="html">   💡 无法显示？点击下载  </content></entry><entry><title>香积寺</title><url>/post/%E9%A6%99%E7%A7%AF%E5%AF%BA/</url><categories/><tags/><content type="html">  香积寺第一次听说香积寺是玩游戏，游戏名字叫《燕云十六声》，以下简称为燕云，故事背景设定为五代十国末期至北宋初年，唐朝是公元618年-907年，五代十国是907年-979年，北宋是960年-1127年。燕云十六声虽然没有复刻这段历史，但是这段历史想必也是这个游戏剧情策划的参考。
香积寺之战的简介时间：唐肃宗至德二年（公元757年）9月
地点：长安城以西，香积寺以北、沣水以东
交战双方：
唐军：广平王李淑、名将郭子仪统帅，联合回纥骑兵
叛军：按时叛军，由安守忠、李归仁等率领
兵力对比
唐军+回纥联军：约15万人
叛军：约10万人
结果：唐军惨胜，收复长安，燕君被斩首六万，俘虏两万，短短四个时辰，两军二十五万人，阵亡十三万人。
有一个很有趣的说法，“香积寺互砍，谁输谁叛军”。今天终于来到了香积寺看一看，正如开篇的香积寺实物图，十分宏伟。进了寺庙后，偶然听到路人说，这里面之前不是现在这个样子，翻修过，更宏伟了，游览的过程中确实看到有正在修缮的房屋。里面有大雄宝殿，还有一个善导塔，有些鼓楼，图书馆，还有免费提供的香，我认为这点非常不错，有些寺庙的香价格昂贵，而实际上香的成本却很低廉。
净土祖庭香积寺作为与少林寺齐名的净土祖庭，也是佛系的顶流存在，寺内的善导塔更是“千年不倒翁”。
后记门口有许多算命的，我站到门口，有个人给我打招呼，辛亏我当时在打电话，我俩并没有进一步交流，我向来是不信这个的。其中有一位，我站在旁边听了听，算命先生说的内容也是一言难尽，但是听者却很认真。我认为这个东西，还在于自己是否相信，要有自己的想法，有自己的价值观与行事准则。算命无非是用别人的道理来指导自己的行为。
奇迹总是一时，命运总是漫长。（摘自诡秘之主）
  </content></entry><entry><title>主页</title><url>/home/main/</url><categories/><tags/><content type="html">     </content></entry><entry><title>泰山之行</title><url>/post/%E6%B3%B0%E5%B1%B1%E4%B9%8B%E8%A1%8C/</url><categories/><tags/><content type="html"><![CDATA[  关于泰山之行——7月5日夜爬泰山一：夜爬泰山1.初到泰山站我是从郑州出发，坐的火车硬卧到达的泰山站，行程五个半小时，因为是卧铺，所以并不累（硬座的话，由于火车硬座的直角设计以及狭小的空间，估计有些吃不消）。下了火车站，映入眼帘的是泰山站三个标志性文字，映衬着泰山，还有周围建筑，看起来很不错。
从泰山站出站口出来，回头看，可以看到一个仿古建筑，如果你是傍晚出站，准备夜爬，那么你将看到下图景色。
出了站就是站前广场，广场上可以存包，买登山杖，买些登山物资什么的，对于登山物资，我建议提前买好一些，感觉泰山站附近的东西都不便宜。登山杖个人感觉没必要买，不过出于好奇买一根也无妨，但是全程下来我觉得用处不大（下山途中有个乘坐客车的地方说什么“公益回收登山杖”，应该真的是公益吧，只是公交车票挺贵的，40元每人，我果断步行下山了）。
2. 出发去红门夜爬泰山，大多数人会选择从红门出发，我也不例外，从泰山站出口到达红门可以乘坐公交车，两块钱，大概40分钟，比较划算。自己打车应该也挺方便，但是路口拉客的出租车司机，说什么5块一个人，但是半路上会拉着你去一个买福牌的店，给你推销，我就是坐了出租车，当然也没强制买，司机师傅也讲了泰山爬山的一些事项。到达红门，遵循司机师傅的建议，不用进入红门游客中心，直接去左侧的阶梯上山。
3. 登山之旅从红门出发，登山路线基本也就确定了，红门-&amp;gt;中天门-&amp;gt;十八盘-&amp;gt;南天门-&amp;gt;碧霞祠-&amp;gt;（五岳独尊，日观峰）-&amp;gt;玉皇顶。红门前往中天门，以及十八盘这两段是夜爬泰山的重头戏，到了南天门，往后的路程就轻松许多。夜晚看不到什么风景，只能看到一些石头上的红色刻字。印象最深的是渐入佳境。 由于前面很多登山路段都是短阶梯+平台的模式，所以爬起来不是很费力，快到南天门有很长的阶梯，一口气爬下来会有些吃力，抬头是长长的台阶，回头是灯光点点的泰安市。 一件觉得有趣的事的是，在这之前拍到了泰山的猫，刚好图片也是向前看的背影和回头看的背影。 到了南天门，人有些多，就不是很想拍照片，有趣的是，就在去年的这天，也就是7月5号，去了老君山的南天门，但是两次的心情与心境完全不一样，一年过去了，&amp;quot;非所经历，不能感悟&amp;quot;。 老君山的南天门 泰山的南天门 再往上，夜里其实也没啥看的，就单纯继续爬，夜里的碧霞祠、五岳独尊、日观峰、玉皇顶 …  ]]></content></entry></search>